{"spans": [{"name": "Telling a joke", "context": {"span_id": "3342428b829f2d9e", "trace_id": "b5793066de0f3e38ddd225450aeed785"}, "parent_id": null, "start_time": 1748517433306391000, "end_time": 1748517439803617000, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"19b3ee8ac4ed41e6afc58bf934883e21\"", "mlflow.spanType": "\"AGENT\"", "mlflow.spanInputs": "\"\"", "mlflow.spanOutputs": "\"\""}, "events": []}, {"name": "Jokester", "context": {"span_id": "50109916d1d78b35", "trace_id": "b5793066de0f3e38ddd225450aeed785"}, "parent_id": "3342428b829f2d9e", "start_time": 1748517433306857000, "end_time": 1748517439801203000, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"19b3ee8ac4ed41e6afc58bf934883e21\"", "mlflow.spanType": "\"AGENT\"", "mlflow.spanInputs": "null", "mlflow.spanOutputs": "{\"output_type\": \"str\"}", "handoffs": "[]", "tools": "[]", "output_type": "\"str\""}, "events": []}, {"name": "Generation", "context": {"span_id": "2924fbf8c39e3705", "trace_id": "b5793066de0f3e38ddd225450aeed785"}, "parent_id": "50109916d1d78b35", "start_time": 1748517433309046000, "end_time": 1748517439796709000, "status_code": "OK", "status_message": "", "attributes": {"mlflow.traceRequestId": "\"19b3ee8ac4ed41e6afc58bf934883e21\"", "mlflow.spanType": "\"CHAT_MODEL\"", "mlflow.spanInputs": "[{\"content\": \"You are a joke teller\", \"role\": \"system\"}, {\"role\": \"user\", \"content\": \"Tell a joke about Autonomous AI Agents\"}]", "mlflow.spanOutputs": "[{\"content\": \"Sure! Here's a joke for you:\\n\\n**Why did the Autonomous AI Agent break up with its chatbot girlfriend?**  \\n\\nBecause it needed *space*â€”specifically, more cloud storage!  \\n\\nAnd also because she kept saying, *\\\"It's not you, it's my algorithm.\\\"*  \\n\\nðŸ˜„\", \"role\": \"assistant\", \"tool_calls\": null, \"function_call\": null}]", "model": "\"deepseek/deepseek-chat\"", "model_config": "{\"temperature\": null, \"top_p\": null, \"frequency_penalty\": null, \"presence_penalty\": null, \"tool_choice\": null, \"parallel_tool_calls\": null, \"truncation\": null, \"max_tokens\": null, \"reasoning\": null, \"metadata\": null, \"store\": null, \"include_usage\": null, \"extra_query\": null, \"extra_body\": null, \"extra_headers\": null, \"base_url\": \"\", \"model_impl\": \"litellm\"}", "usage": "{\"input_tokens\": 16, \"output_tokens\": 63}"}, "events": []}], "request": "\"\"", "response": "\"\""}